# Machine Learning & Analytics Patterns
#
# Patterns for ML workflows, feature engineering, model evaluation,
# and data analytics operations.

apiVersion: loom/v1
kind: PatternLibrary

metadata:
  name: ml-analytics-patterns
  description: ML and analytics workflow patterns
  version: "1.0"
  tags:
    - ml
    - analytics
    - data-science
    - statistics

patterns:
  # Feature Engineering
  - name: feature_extraction
    description: Extract and transform features for ML models
    intent_keywords:
      - extract features
      - feature engineering
      - transform data
      - create features

    template: |
      Generate features from raw data for ML models.

      Steps:
      1. Identify raw data columns
      2. Apply transformations:
        - Numeric: normalization, standardization, binning
        - Categorical: encoding, one-hot, embeddings
        - Temporal: extract day, hour, dayofweek, seasonal
        - Text: tokenization, TF-IDF, embeddings
      3. Handle missing values
      4. Create interaction features if needed
      5. Generate SQL or transformation pipeline

    examples:
      - input: "Extract time-based features from orders"
        output: |
          SELECT
            order_id,
            customer_id,
            order_date,
            EXTRACT(DOW FROM order_date) as day_of_week,
            EXTRACT(HOUR FROM order_date) as hour_of_day,
            EXTRACT(MONTH FROM order_date) as month,
            CASE WHEN EXTRACT(DOW FROM order_date) IN (0, 6) THEN 1 ELSE 0 END as is_weekend,
            total_amount,
            LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,
            DATEDIFF(order_date, LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date)) as days_since_last_order
          FROM orders

      - input: "Create customer behavior features"
        output: |
          SELECT
            customer_id,
            COUNT(*) as total_orders,
            SUM(total_amount) as total_spend,
            AVG(total_amount) as avg_order_value,
            MAX(total_amount) as max_order_value,
            MIN(order_date) as first_order_date,
            MAX(order_date) as last_order_date,
            DATEDIFF(MAX(order_date), MIN(order_date)) as customer_lifetime_days,
            COUNT(DISTINCT EXTRACT(MONTH FROM order_date)) as active_months
          FROM orders
          GROUP BY customer_id

  # Statistical Analysis
  - name: descriptive_statistics
    description: Calculate descriptive statistics for data exploration
    intent_keywords:
      - calculate stats
      - descriptive statistics
      - summary statistics
      - data distribution

    template: |
      Generate statistical summaries of data.

      Steps:
      1. Identify numeric columns
      2. Calculate:
        - Central tendency: mean, median, mode
        - Dispersion: std dev, variance, range, IQR
        - Distribution: min, max, quartiles, percentiles
        - Shape: skewness, kurtosis
      3. Generate SQL with aggregate functions
      4. Format results in readable table

    examples:
      - input: "Get statistics for product prices"
        output: |
          SELECT
            'price' as metric,
            COUNT(*) as count,
            AVG(price) as mean,
            STDDEV(price) as std_dev,
            MIN(price) as min,
            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY price) as q1,
            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY price) as median,
            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY price) as q3,
            MAX(price) as max
          FROM products

      - input: "Analyze order distribution by region"
        output: |
          SELECT
            region,
            COUNT(*) as order_count,
            SUM(total_amount) as total_revenue,
            AVG(total_amount) as avg_order_value,
            STDDEV(total_amount) as stddev_order_value,
            MIN(total_amount) as min_order,
            MAX(total_amount) as max_order
          FROM orders
          GROUP BY region
          ORDER BY total_revenue DESC

  # Cohort Analysis
  - name: cohort_analysis
    description: Analyze user cohorts over time
    intent_keywords:
      - cohort analysis
      - retention analysis
      - cohort retention
      - user cohorts

    template: |
      Perform cohort-based retention analysis.

      Steps:
      1. Define cohort (e.g., signup month)
      2. Track metric over time (e.g., monthly active users)
      3. Calculate retention rates per cohort
      4. Generate cohort matrix
      5. Visualize retention curves

    examples:
      - input: "Calculate monthly cohort retention"
        output: |
          WITH cohorts AS (
            SELECT
              user_id,
              DATE_TRUNC('month', signup_date) as cohort_month
            FROM users
          ),
          activity AS (
            SELECT
              user_id,
              DATE_TRUNC('month', activity_date) as activity_month
            FROM user_activity
          )
          SELECT
            c.cohort_month,
            a.activity_month,
            DATEDIFF('month', c.cohort_month, a.activity_month) as month_number,
            COUNT(DISTINCT a.user_id) as active_users,
            COUNT(DISTINCT a.user_id) * 100.0 / COUNT(DISTINCT c.user_id) as retention_rate
          FROM cohorts c
          LEFT JOIN activity a ON c.user_id = a.user_id
          GROUP BY c.cohort_month, a.activity_month
          ORDER BY c.cohort_month, month_number

  # A/B Test Analysis
  - name: ab_test_analysis
    description: Analyze A/B test results for statistical significance
    intent_keywords:
      - ab test
      - split test
      - test significance
      - variant comparison

    template: |
      Evaluate A/B test performance and statistical significance.

      Steps:
      1. Segment users by variant (A vs B)
      2. Calculate conversion rates per variant
      3. Compute statistical significance (chi-square, t-test)
      4. Calculate confidence intervals
      5. Determine winner and effect size

    examples:
      - input: "Compare conversion rates between variants"
        output: |
          SELECT
            variant,
            COUNT(*) as total_users,
            SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) as conversions,
            SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as conversion_rate,
            AVG(revenue) as avg_revenue_per_user,
            SUM(revenue) as total_revenue
          FROM ab_test_results
          WHERE test_id = 'homepage_redesign'
          GROUP BY variant

      - input: "Calculate test statistical significance"
        output: |
          WITH variant_stats AS (
            SELECT
              variant,
              COUNT(*) as n,
              SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) as conversions,
              SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) * 1.0 / COUNT(*) as p
            FROM ab_test_results
            WHERE test_id = 'homepage_redesign'
            GROUP BY variant
          )
          SELECT
            a.variant as variant_a,
            b.variant as variant_b,
            a.p as conversion_rate_a,
            b.p as conversion_rate_b,
            (b.p - a.p) / a.p * 100 as lift_pct,
            -- Z-score calculation for significance
            (b.p - a.p) / SQRT(((a.p * (1 - a.p) / a.n) + (b.p * (1 - b.p) / b.n))) as z_score
          FROM variant_stats a, variant_stats b
          WHERE a.variant = 'A' AND b.variant = 'B'

  # Anomaly Detection
  - name: anomaly_detection
    description: Detect statistical anomalies in time series data
    intent_keywords:
      - detect anomalies
      - find outliers
      - anomaly detection
      - outlier detection

    template: |
      Identify anomalous data points using statistical methods.

      Steps:
      1. Calculate baseline statistics (mean, stddev)
      2. Define threshold (e.g., 3 standard deviations)
      3. Identify points beyond threshold
      4. Apply methods:
        - Z-score
        - IQR (interquartile range)
        - Moving average deviation
      5. Flag anomalies with severity

    examples:
      - input: "Find anomalies in daily sales"
        output: |
          WITH stats AS (
            SELECT
              AVG(daily_sales) as mean_sales,
              STDDEV(daily_sales) as stddev_sales
            FROM daily_metrics
            WHERE date >= CURRENT_DATE - INTERVAL '90 days'
          )
          SELECT
            date,
            daily_sales,
            s.mean_sales,
            s.stddev_sales,
            (daily_sales - s.mean_sales) / s.stddev_sales as z_score,
            CASE
              WHEN ABS((daily_sales - s.mean_sales) / s.stddev_sales) > 3 THEN 'severe'
              WHEN ABS((daily_sales - s.mean_sales) / s.stddev_sales) > 2 THEN 'moderate'
              ELSE 'normal'
            END as anomaly_level
          FROM daily_metrics
          CROSS JOIN stats s
          WHERE date >= CURRENT_DATE - INTERVAL '90 days'
            AND ABS((daily_sales - s.mean_sales) / s.stddev_sales) > 2
          ORDER BY ABS((daily_sales - s.mean_sales) / s.stddev_sales) DESC

  # Funnel Analysis
  - name: funnel_analysis
    description: Analyze conversion funnels and drop-off points
    intent_keywords:
      - funnel analysis
      - conversion funnel
      - drop-off analysis
      - funnel metrics

    template: |
      Track user progression through conversion funnel.

      Steps:
      1. Define funnel stages (e.g., visit -> signup -> purchase)
      2. Count users at each stage
      3. Calculate conversion rates between stages
      4. Identify drop-off points
      5. Segment by user attributes

    examples:
      - input: "Analyze signup to purchase funnel"
        output: |
          WITH funnel AS (
            SELECT
              user_id,
              MAX(CASE WHEN event = 'visit' THEN 1 ELSE 0 END) as visited,
              MAX(CASE WHEN event = 'signup' THEN 1 ELSE 0 END) as signed_up,
              MAX(CASE WHEN event = 'add_to_cart' THEN 1 ELSE 0 END) as added_to_cart,
              MAX(CASE WHEN event = 'checkout' THEN 1 ELSE 0 END) as checked_out,
              MAX(CASE WHEN event = 'purchase' THEN 1 ELSE 0 END) as purchased
            FROM user_events
            WHERE event_date >= CURRENT_DATE - INTERVAL '30 days'
            GROUP BY user_id
          )
          SELECT
            'visit' as stage,
            SUM(visited) as users,
            100.0 as conversion_rate
          FROM funnel
          UNION ALL
          SELECT
            'signup' as stage,
            SUM(signed_up) as users,
            SUM(signed_up) * 100.0 / SUM(visited) as conversion_rate
          FROM funnel
          UNION ALL
          SELECT
            'add_to_cart' as stage,
            SUM(added_to_cart) as users,
            SUM(added_to_cart) * 100.0 / SUM(signed_up) as conversion_rate
          FROM funnel
          UNION ALL
          SELECT
            'checkout' as stage,
            SUM(checked_out) as users,
            SUM(checked_out) * 100.0 / SUM(added_to_cart) as conversion_rate
          FROM funnel
          UNION ALL
          SELECT
            'purchase' as stage,
            SUM(purchased) as users,
            SUM(purchased) * 100.0 / SUM(checked_out) as conversion_rate
          FROM funnel

  # RFM Analysis
  - name: rfm_analysis
    description: Calculate RFM (Recency, Frequency, Monetary) scores
    intent_keywords:
      - rfm analysis
      - customer segmentation
      - rfm score
      - customer value

    template: |
      Segment customers by RFM metrics.

      Steps:
      1. Calculate Recency (days since last purchase)
      2. Calculate Frequency (number of purchases)
      3. Calculate Monetary (total spend)
      4. Score each dimension (1-5 scale)
      5. Create RFM segments

    examples:
      - input: "Calculate RFM scores for customers"
        output: |
          WITH rfm AS (
            SELECT
              customer_id,
              DATEDIFF(CURRENT_DATE, MAX(order_date)) as recency,
              COUNT(*) as frequency,
              SUM(total_amount) as monetary
            FROM orders
            WHERE order_date >= CURRENT_DATE - INTERVAL '365 days'
            GROUP BY customer_id
          )
          SELECT
            customer_id,
            recency,
            frequency,
            monetary,
            NTILE(5) OVER (ORDER BY recency DESC) as recency_score,
            NTILE(5) OVER (ORDER BY frequency) as frequency_score,
            NTILE(5) OVER (ORDER BY monetary) as monetary_score,
            CONCAT(
              NTILE(5) OVER (ORDER BY recency DESC),
              NTILE(5) OVER (ORDER BY frequency),
              NTILE(5) OVER (ORDER BY monetary)
            ) as rfm_score
          FROM rfm
          ORDER BY monetary DESC
