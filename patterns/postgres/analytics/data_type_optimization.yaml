# === METADATA START ===
name: data_type_optimization
title: "Data Type Optimization"
description: "Recommends better column data types for storage and performance by analyzing actual data usage and suggesting more efficient alternatives"
category: analytics
difficulty: intermediate
backend_type: postgres
priority: 65
# === METADATA END ===

# === USE_CASES START ===
use_cases:
  - "Reduce table size by 20-60%"
  - "Improve query performance"
  - "Optimize index size"
  - "Lower storage costs"
  - "Speed up backups and replication"
  - "Identify oversized VARCHAR columns"
  - "Detect unnecessary precision in NUMERIC types"
# === USE_CASES END ===

# === PARAMETERS START ===
parameters:
  - name: table_name
    type: string
    required: true
    description: "Table to analyze for data type optimization"
    example: "users"

  - name: schema_name
    type: string
    required: false
    description: "Schema name (defaults to public)"
    default: "public"
    example: "public"
# === PARAMETERS END ===

# === TEMPLATES START ===
templates:
  analysis: |
    -- Data Type Analysis for {{.schema_name}}.{{.table_name}}

    -- Step 1: Get current column definitions
    SELECT
      column_name,
      data_type,
      character_maximum_length,
      numeric_precision,
      numeric_scale,
      is_nullable,
      column_default
    FROM information_schema.columns
    WHERE table_schema = '{{.schema_name}}'
      AND table_name = '{{.table_name}}'
    ORDER BY ordinal_position;

    -- Step 2: Analyze actual data usage for VARCHAR/TEXT columns
    SELECT
      column_name,
      data_type,
      character_maximum_length AS declared_max,
      MAX(LENGTH(column_name::text)) AS actual_max,
      AVG(LENGTH(column_name::text))::INT AS avg_length,
      MIN(LENGTH(column_name::text)) AS min_length
    FROM {{.schema_name}}.{{.table_name}},
         information_schema.columns c
    WHERE c.table_schema = '{{.schema_name}}'
      AND c.table_name = '{{.table_name}}'
      AND c.data_type IN ('character varying', 'text')
    GROUP BY column_name, data_type, character_maximum_length;

    -- Step 3: Analyze numeric ranges
    -- Replace column_name with actual numeric columns
    SELECT
      'column_name' AS column,
      MIN(column_name) AS min_value,
      MAX(column_name) AS max_value,
      AVG(column_name) AS avg_value
    FROM {{.schema_name}}.{{.table_name}};

    -- Step 4: Check table size impact
    SELECT
      pg_size_pretty(pg_total_relation_size('{{.schema_name}}.{{.table_name}}')) AS total_size,
      pg_size_pretty(pg_relation_size('{{.schema_name}}.{{.table_name}}')) AS table_size,
      pg_size_pretty(pg_indexes_size('{{.schema_name}}.{{.table_name}}')) AS indexes_size;

  recommendations: |
    -- Data Type Optimization Recommendations

    -- Common optimizations:

    -- 1. VARCHAR(255) → VARCHAR(n) where n is actual max length + margin
    -- Example: email column max length is 45, use VARCHAR(100)
    -- Savings: Minimal for heap, but significant for indexes

    -- 2. TEXT → VARCHAR(n) when length is bounded
    -- TEXT has no declared limit, VARCHAR with limit enables optimizations
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN column_name TYPE VARCHAR(100);

    -- 3. INTEGER (4 bytes) → SMALLINT (2 bytes) for small ranges
    -- SMALLINT range: -32,768 to 32,767
    -- Use for: age, quantity, status codes
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN age TYPE SMALLINT;

    -- 4. BIGINT (8 bytes) → INTEGER (4 bytes) when range permits
    -- INTEGER range: -2 billion to 2 billion
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN count TYPE INTEGER;

    -- 5. NUMERIC/DECIMAL → INTEGER for whole numbers
    -- Example: quantity stored as NUMERIC(10,0) → INTEGER
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN quantity TYPE INTEGER
      USING quantity::INTEGER;

    -- 6. TIMESTAMP → DATE for date-only data
    -- DATE: 4 bytes vs TIMESTAMP: 8 bytes
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN birth_date TYPE DATE;

    -- 7. BOOLEAN over VARCHAR(1) or CHAR(1)
    -- BOOLEAN: 1 byte, more expressive
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN is_active TYPE BOOLEAN
      USING CASE WHEN is_active = 'Y' THEN TRUE ELSE FALSE END;

    -- 8. JSONB over TEXT for JSON data
    -- JSONB supports indexing and efficient queries
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ALTER COLUMN metadata TYPE JSONB
      USING metadata::JSONB;

  size_calculation: |
    -- Calculate storage savings from data type changes

    -- Current table size
    SELECT
      relname AS table_name,
      pg_size_pretty(pg_total_relation_size(oid)) AS total_size,
      pg_size_pretty(pg_relation_size(oid)) AS table_size,
      pg_size_pretty(pg_indexes_size(oid)) AS indexes_size,
      n_live_tup AS row_count
    FROM pg_class
    WHERE relname = '{{.table_name}}';

    -- Estimate savings example:
    -- If converting 10 columns from INTEGER (4 bytes) to SMALLINT (2 bytes)
    -- Savings per row: 10 * 2 bytes = 20 bytes
    -- For 10M rows: 20 * 10,000,000 = 200 MB

    -- Calculate actual row size
    SELECT
      pg_column_size(row(t.*)) AS row_size_bytes,
      COUNT(*) AS row_count,
      pg_size_pretty(AVG(pg_column_size(row(t.*)))::BIGINT * COUNT(*)) AS estimated_table_size
    FROM {{.schema_name}}.{{.table_name}} t;

  migration: |
    -- Safe migration process for data type changes

    -- Step 1: Validate data fits in new type
    -- Example: Check INTEGER values fit in SMALLINT
    SELECT
      COUNT(*) AS total_rows,
      COUNT(*) FILTER (WHERE column_name < -32768 OR column_name > 32767) AS out_of_range
    FROM {{.schema_name}}.{{.table_name}};

    -- Step 2: Create new column with optimized type
    ALTER TABLE {{.schema_name}}.{{.table_name}}
      ADD COLUMN column_name_new SMALLINT;

    -- Step 3: Copy data with conversion
    UPDATE {{.schema_name}}.{{.table_name}}
      SET column_name_new = column_name::SMALLINT;

    -- Step 4: Verify data integrity
    SELECT
      COUNT(*) AS total,
      COUNT(*) FILTER (WHERE column_name::SMALLINT = column_name_new) AS matching,
      COUNT(*) FILTER (WHERE column_name::SMALLINT != column_name_new OR column_name_new IS NULL) AS mismatches
    FROM {{.schema_name}}.{{.table_name}};

    -- Step 5: Drop old column and rename new one
    BEGIN;
      ALTER TABLE {{.schema_name}}.{{.table_name}} DROP COLUMN column_name;
      ALTER TABLE {{.schema_name}}.{{.table_name}} RENAME COLUMN column_name_new TO column_name;
    COMMIT;

    -- Step 6: Rebuild indexes
    REINDEX TABLE {{.schema_name}}.{{.table_name}};

    -- Step 7: Update statistics
    ANALYZE {{.schema_name}}.{{.table_name}};
# === TEMPLATES END ===

# === EXAMPLES START ===
examples:
  - name: "Oversized VARCHAR columns"
    parameters:
      table_name: "users"
      schema_name: "public"
    expected_result: |
      Analysis shows:
      - email VARCHAR(255): actual max 52 chars, avg 28 chars
      - username VARCHAR(255): actual max 18 chars, avg 12 chars
      - first_name VARCHAR(255): actual max 32 chars, avg 8 chars

      Recommendations:
      ALTER TABLE users
        ALTER COLUMN email TYPE VARCHAR(100),
        ALTER COLUMN username TYPE VARCHAR(50),
        ALTER COLUMN first_name TYPE VARCHAR(50);

      Estimated savings:
      - Table size: 5% reduction (minimal for heap storage)
      - Index size: 40% reduction on email index (significant!)
      - Total: ~500 MB saved on indexes for 10M rows

  - name: "Integer type downsizing"
    parameters:
      table_name: "orders"
      schema_name: "public"
    expected_result: |
      Analysis shows:
      - quantity INTEGER: range 1-50 (max value ever: 50)
      - age_days INTEGER: range 0-365 (days old)
      - status_code INTEGER: range 1-10 (enum-like)

      Recommendations:
      ALTER TABLE orders
        ALTER COLUMN quantity TYPE SMALLINT,
        ALTER COLUMN age_days TYPE SMALLINT,
        ALTER COLUMN status_code TYPE SMALLINT;

      Estimated savings:
      - 6 bytes per row (3 columns × 2 bytes saved)
      - For 50M rows: 300 MB table savings
      - Index savings: ~200 MB additional

  - name: "Numeric precision reduction"
    parameters:
      table_name: "products"
      schema_name: "public"
    expected_result: |
      Analysis shows:
      - price NUMERIC(18,6): actual precision needed NUMERIC(10,2)
      - weight NUMERIC(18,6): whole numbers only
      - discount_percent NUMERIC(18,6): range 0-100 with 1 decimal

      Recommendations:
      ALTER TABLE products
        ALTER COLUMN price TYPE NUMERIC(10,2),
        ALTER COLUMN weight TYPE INTEGER,
        ALTER COLUMN discount_percent TYPE NUMERIC(4,1);

      Storage per row before: 16 + 16 + 16 = 48 bytes
      Storage per row after: 8 + 4 + 4 = 16 bytes
      Savings: 32 bytes × 1M rows = 32 MB

  - name: "Date/Time optimization"
    parameters:
      table_name: "events"
      schema_name: "public"
    expected_result: |
      Analysis shows:
      - birth_date TIMESTAMP: time component always 00:00:00
      - hire_date TIMESTAMP WITH TIME ZONE: timezone not used
      - anniversary_date TIMESTAMP: date-only data

      Recommendations:
      ALTER TABLE events
        ALTER COLUMN birth_date TYPE DATE,
        ALTER COLUMN hire_date TYPE DATE,
        ALTER COLUMN anniversary_date TYPE DATE;

      Savings:
      - TIMESTAMP: 8 bytes → DATE: 4 bytes (50% reduction)
      - For 5M rows: 60 MB saved (3 columns × 4 bytes × 5M)
# === EXAMPLES END ===

# === BEST_PRACTICES START ===
best_practices: |
  ## Data Type Optimization Best Practices

  ### 1. Measure Before Optimizing
  ```sql
  -- Check actual data ranges
  SELECT MIN(column), MAX(column), AVG(column) FROM table;

  -- Check actual string lengths
  SELECT MAX(LENGTH(column)), AVG(LENGTH(column)) FROM table;
  ```

  ### 2. Integer Type Selection
  **Type sizes:**
  - SMALLINT: 2 bytes, range -32K to 32K
  - INTEGER: 4 bytes, range -2B to 2B
  - BIGINT: 8 bytes, range -9 quintillion to 9 quintillion

  **Rules:**
  - Age, small quantities: SMALLINT
  - Most IDs, counts: INTEGER
  - Very large IDs (Twitter snowflake): BIGINT

  ### 3. VARCHAR vs TEXT
  **Use VARCHAR(n) when:**
  - Maximum length is known (email, username, phone)
  - Enables index optimization
  - Better for constraints and validation

  **Use TEXT when:**
  - Content is unbounded (comments, descriptions)
  - Highly variable length

  **Avoid VARCHAR(255) unless necessary:**
  - Common default, often oversized
  - No performance benefit over smaller lengths

  ### 4. NUMERIC vs INTEGER
  ```sql
  -- Bad: NUMERIC for whole numbers
  price NUMERIC(10,0)  -- 8-16 bytes

  -- Good: INTEGER for whole numbers
  price INTEGER  -- 4 bytes
  ```

  **Use NUMERIC only for:**
  - Decimal precision required (currency with cents)
  - Very large numbers beyond BIGINT range

  ### 5. TIMESTAMP vs DATE
  **TIMESTAMP (8 bytes):**
  - Full date and time
  - Use when time matters

  **DATE (4 bytes):**
  - Date only, no time component
  - Use for birthdays, hire dates, expiration dates
  - 50% storage savings

  ### 6. BOOLEAN over VARCHAR/CHAR
  ```sql
  -- Bad: VARCHAR for boolean
  is_active VARCHAR(1)  -- 'Y' or 'N', wastes space

  -- Good: Actual BOOLEAN
  is_active BOOLEAN  -- 1 byte, more expressive
  ```

  ### 7. Safe Migration Process
  1. **Analyze**: Verify all data fits in new type
  2. **Add column**: Create new column with target type
  3. **Copy data**: Populate new column
  4. **Verify**: Check data integrity
  5. **Swap**: Drop old, rename new
  6. **Rebuild**: REINDEX and ANALYZE

  ### 8. Consider Index Impact
  Data type changes affect indexes more than heap storage:
  - VARCHAR(255) → VARCHAR(50): Index savings > table savings
  - INTEGER → SMALLINT: Index becomes half the size

  ### 9. Test Performance Impact
  Not all optimizations improve performance:
  - Smaller types = less I/O (good)
  - Type casting overhead (bad)
  - Measure actual query performance

  ### 10. Memory Layers for Schema Optimization
  - **Kernel Layer**: Cache table schemas and column metadata for quick reference during analysis
  - **L1 Cache**: Keep recent data type analysis results (last 5-8 tables) showing actual ranges and recommendations
  - **L2 Compressed**: Archive schema optimization patterns and storage savings from earlier tables
  - **Swap Layer**: Store complete schema evolution history; use recall_conversation to reference similar type optimization decisions from previous projects
  This enables consistent type selection patterns and tracking cumulative storage savings across database optimization efforts.
# === BEST_PRACTICES END ===

# === RELATED_PATTERNS START ===
related_patterns:
  - missing_index_analysis
  - vacuum_recommendation
  - partition_recommendation
# === RELATED_PATTERNS END ===

validation:
  rules:
    - "Check actual data length/range fits in new type"
    - "Verify no data loss with new type (run validation query)"
    - "Estimate storage savings (must be worth migration effort)"
    - "Test performance impact on key queries"
    - "Update application code if type changes affect casting"
